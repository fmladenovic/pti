# 游쐭lackjack uporedjivanje algoritama

Uporediti Monte-Karlo algoritam sa SARSA i Q-LEARNING algoritmima.

## Implementacija re코enja
Budu캖i da nam nije toliko bitno da odmah a쬿riramo trenutne *q* vrednosti (nije mogu캖e da nam se ponovi isto stanje dva puta u jednoj partiji) a쬿riranje je vr코eno na kraju svake partije sa odgovaraju캖im vrednostima - Nagrada za svaki potez koji nije na코 krajnji potez u partiji je 0, a nagrada za krajnji potez je informacija o tome da li smo pobedili (+1), izgubili (-1) ili odigrali nere코eno (0).

### SARSA

Za stanje *s_prim* i akciju *a_prim* uzimaju se naredno stanje iz na코e partije, a za akciju se bira ona 	akcija koju je proizvela na코a politika odlu캜ivanja. Terminalna stanja imaju vrednost 0, zato 코to mi ne donosimo odluku u njima. 
A쬿riranje *q* vrednosti se vr코i slede캖im algoritmom:

	def  update_q_sarsa(self, s, a, s_prim, a_prim, r, l = LAMBDA, g = GAMA):
		key = self._convert_to_key(s, a)
		next_q = 0

		if  key  in  self.sa_q.keys(): # Terminal value will be 0
			next_q = self.q(s_prim, a_prim)

		old_q = self.q(s, a)
		new_q = old_q + l * ( r + g * next_q - old_q)
		self.sa_q[key] = new_q
	

### Q learning

*Q learning* algoritam se razlikuje od *SARSA* algoritma samo po tome 코to te쬴mo da u narednom stanju izaberemo najbolju akciju. 
A쬿riranje *q* vrednosti se vr코i slede캖im algoritmom:

	def  update_q_q_learning(self, s, a, s_prim, r, l = LAMBDA, g = GAMA):
		key = self._convert_to_key(s, a)
		next_q = 0

		if  key  in  self.sa_q.keys():
			max_a = greedy(self, s_prim)
			next_q = self.q(s_prim, max_a)

		old_q = self.q(s, a)
		new_q = old_q + l * ( r + g * next_q - old_q)
		self.sa_q[key] = new_q

## Rezultati
Nakon 2,000,000 iteracija u캜enja *q* vrednosti, rezultati partija su slede캖i:
<table>
<tr> 
<td>  Parties </td>
 <td> 10,000 </td> 
 <td> 30,000 </td> 
 <td> 50,000 </td> 
 <td> 100,000</td> 
 </tr>
<tr> 
<td> Monte-Karlo  </td>
<td> w:2343 t:688 l:6969 </td> 
<td> w:7027 t:2075 l:20898 </td> 
<td>  w:11676 t:3390 l:34934</td> 
<td> w:233825 t:67141 l:699034 </td> 
</tr>
<tr> 
<td> SARSA  </td>
<td> w:2298 t:717 l:6985</td> 
<td> w:6931 t:2161 l:20908</td> 
<td>w:11474 t:3575 l:34951 </td> 
<td> w:229626 t:71132 l:699242 </td> 
</tr>
<tr> 
<td> Q-learning </td>
<td> w:2336 t:667 l:6997</td> 
<td>w:6898  t:2098 l:21004</td> 
<td> w:11656  t:3472 l:34872</td> 
<td> w:233468 t:68923 l:697609</td> 
</tr>
</table>

Pobolj코anje algoritama je mogu캖e uz pametnu  inicijalizaciju q-vrednosti i pode코avanja hiper-parametara *lambda* i *gama*.